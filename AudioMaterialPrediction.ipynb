{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/shashi/.cache/torch/hub/harritaylor_torchvggish_master\n",
      "Downloading: \"https://github.com/harritaylor/torchvggish/releases/download/v0.1/vggish-10086976.pth\" to /Users/shashi/.cache/torch/hub/checkpoints/vggish-10086976.pth\n",
      "100%|██████████| 275M/275M [00:31<00:00, 9.20MB/s] \n",
      "Downloading: \"https://github.com/harritaylor/torchvggish/releases/download/v0.1/vggish_pca_params-970ea276.pth\" to /Users/shashi/.cache/torch/hub/checkpoints/vggish_pca_params-970ea276.pth\n",
      "100%|██████████| 177k/177k [00:00<00:00, 3.31MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGGish(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (embeddings): Sequential(\n",
       "    (0): Linear(in_features=12288, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=4096, out_features=128, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (pproc): Postprocessor()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "vggish_model = torch.hub.load('harritaylor/torchvggish', 'vggish')\n",
    "vggish_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "url, filename = (\"http://soundbible.com/grab.php?id=1698&type=wav\", \"bus_chatter.wav\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19, 128])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vggish_model.forward(filename).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  733\n",
      "Number of testing samples:  244\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(729, 243)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "times_info = {}\n",
    "for file in glob.glob(\"./vis-data-256/*_times.txt\"):\n",
    "    with open(file, \"r\") as f:\n",
    "        data = f.readlines()\n",
    "        data = [line.strip().split() for line in data]\n",
    "        data = [(float(line[0]), line[1]) for line in data if line[1] != \"None\"]\n",
    "        times_info[file.split(\"/\")[-1].split(\"_\")[0]] = data\n",
    "\n",
    "train_file = \"./vis-data-256/train.txt\"\n",
    "train_info = []\n",
    "\n",
    "with open(train_file, \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    data = [line.strip().split() for line in data]\n",
    "    print(\"Number of training samples: \", len(data))\n",
    "    for d in data:\n",
    "        if d[0] not in train_info:\n",
    "            train_info.append(d[0])\n",
    "\n",
    "test_file = \"./vis-data-256/test.txt\"\n",
    "with open(test_file, \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    data = [line.strip().split() for line in data]\n",
    "    print(\"Number of testing samples: \", len(data))\n",
    "\n",
    "#Focus only on one frame in a video\n",
    "times_info_updated_train = {}\n",
    "all_labels = []\n",
    "train_data = []\n",
    "\n",
    "for key, value in times_info.items():\n",
    "    if key in train_info:\n",
    "        if key not in times_info_updated_train:\n",
    "            if len(value) > 1:\n",
    "                times_info_updated_train[key] = value[0]\n",
    "                all_labels.append(value[1][1])\n",
    "                train_data.append((key, value[1][1]))\n",
    "\n",
    "times_info_updated_test = {}\n",
    "test_data = []\n",
    "\n",
    "for key, value in times_info.items():\n",
    "    if key not in train_info:\n",
    "        if key not in times_info_updated_test:\n",
    "            if len(value) > 1:\n",
    "                times_info_updated_test[key] = value[0]\n",
    "                test_data.append((key, value[1][1]))\n",
    "                \n",
    "len(times_info_updated_train), len(times_info_updated_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(729, 243)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "integer_labels = label_encoder.fit_transform(all_labels)\n",
    "\n",
    "tensor_labels = torch.from_numpy(integer_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, audio_dir):\n",
    "        self.data = data\n",
    "        self.audio_dir = audio_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_file_path = os.path.join(self.audio_dir, self.data[idx][0] + \".wav\")\n",
    "        waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "        label = torch.tensor(label_encoder.transform([self.data[idx][1]]), dtype=torch.long)\n",
    "        return audio_file_path, label\n",
    "    \n",
    "train_dataset = CustomDataset(data=train_data, audio_dir='./train_audio')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "test_dataset = CustomDataset(data=test_data, audio_dir='./test_audio')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        self.pooling = nn.AdaptiveAvgPool2d((1, 128))\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pooling(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/shashi/.cache/torch/hub/harritaylor_torchvggish_master\n",
      "100%|██████████| 50/50 [1:45:31<00:00, 126.62s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AudioClassifier(num_classes=50)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "vggish_model = torch.hub.load('harritaylor/torchvggish', 'vggish')\n",
    "vggish_model.eval()\n",
    "\n",
    "for epoch in tqdm(range(50)):\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = []\n",
    "        \n",
    "        for audio_file_path, label in zip(batch[0], batch[1]):\n",
    "            x = vggish_model.forward(audio_file_path)\n",
    "            x = x.unsqueeze(0)\n",
    "            output = model(x)\n",
    "            outputs.append(output)\n",
    "        outputs = torch.cat(outputs) \n",
    "        labels = batch[1].squeeze()\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 10.699588477366255\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()  \n",
    "with torch.no_grad():  \n",
    "    for batch in test_dataloader:\n",
    "        outputs = []\n",
    "        labels = []\n",
    "        for audio_file_path, label in zip(batch[0], batch[1]):\n",
    "            x = vggish_model.forward(audio_file_path)\n",
    "            x = x.unsqueeze(0)\n",
    "            output = model(x)\n",
    "            outputs.append(output)\n",
    "            labels.append(label)\n",
    "\n",
    "        outputs = torch.cat(outputs) \n",
    "        labels = torch.stack(labels).squeeze()\n",
    "        _, predicted = torch.max(outputs, 1)  \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy =  correct / total\n",
    "print('Accuracy:', accuracy*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
