{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision.io\n",
    "import glob\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./vis-data-256 contains <datetime>_times.txt and <datetime>_denoised.wav, and <datetime>_denoised.mp4\n",
    "# We will use the times.txt to get the timestamps of the frames in the video, a 1 second window of audio around each frame, and the material name\n",
    "# times.txt has the format \"<timestamp> <material name> <hit_type_1> <hit_type_2>\" for each line\n",
    "\n",
    "# Load the times.txt files in a dictionary with key as timestamp and value as a list with tuples of timestamp and material name\n",
    "times_info = {}\n",
    "for file in glob.glob(\"./vis-data-256/*_times.txt\"):\n",
    "    with open(file, \"r\") as f:\n",
    "        data = f.readlines()\n",
    "        data = [line.strip().split() for line in data]\n",
    "        data = [(float(line[0]), line[1]) for line in data if line[1] != \"None\"]\n",
    "        times_info[file.split(\"/\")[-1].split(\"_\")[0]] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-09-30-20-56-02-193\n",
      "[(1.647073, 'water'), (2.963323, 'water'), (4.252625, 'water'), (4.522365, 'water'), (5.877479, 'water'), (6.228427, 'water'), (6.499427, 'water'), (7.439615, 'water'), (7.719083, 'water'), (8.008896, 'water'), (8.301615, 'water'), (8.708354, 'water'), (8.989927, 'water'), (9.634604, 'water'), (10.109011, 'water'), (10.771895, 'water'), (11.047698, 'water'), (11.40924, 'water'), (11.687771, 'water'), (12.259958, 'water'), (12.572865, 'water'), (13.134896, 'water'), (13.461678, 'water'), (14.4075, 'water'), (14.718781, 'water'), (15.029875, 'water'), (15.304459, 'water'), (15.683594, 'water'), (15.936271, 'water'), (16.292042, 'water'), (16.600178, 'water'), (17.222605, 'water'), (17.614635, 'water'), (17.918615, 'water'), (18.271875, 'water'), (18.552458, 'water'), (19.007177, 'water'), (19.287563, 'water'), (19.685938, 'water'), (19.938583, 'water'), (20.303207, 'water'), (20.57099, 'water'), (22.161552, 'water'), (22.418261, 'water'), (22.767897, 'water'), (23.386312, 'water'), (23.682188, 'water'), (24.053207, 'water'), (24.97702, 'water')]\n"
     ]
    }
   ],
   "source": [
    "for k,v in times_info.items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wav file from \"./vis-data-256/2015-09-30-20-56-02-193_denoised.wav\"\n",
    "aud_data, sr = torchaudio.load(\"./vis-data-256/2015-09-30-20-56-02-193_denoised.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of the audio file in seconds\n",
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n",
      "2.0.2+cu117\n",
      "0.15.2+cu117\n"
     ]
    }
   ],
   "source": [
    "# Versions\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_reader = torchvision.io.VideoReader(\"./vis-data-256/2015-09-30-20-56-02-193_denoised.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    }
   ],
   "source": [
    "frames, audio, metadata = torchvision.io.read_video(\"./vis-data-256/2015-09-30-20-56-02-193_denoised.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([841, 256, 456, 3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing IO for audio and video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    }
   ],
   "source": [
    "for date_time, frame_info in times_info.items():\n",
    "    # Load the audio and video\n",
    "    # video = torch.load(f\"./vis-data-256/{date_time}_denoised.mp4\")\n",
    "    frames, audio, metadata = torchvision.io.read_video(f\"./vis-data-256/{date_time}_denoised.mp4\")\n",
    "    audio = audio[0]\n",
    "    video_rate = metadata[\"video_fps\"]\n",
    "    audio_rate = metadata[\"audio_fps\"]\n",
    "\n",
    "    # Get the timestamps of the frames in the video\n",
    "    frame_timestamps = [frame[0] for frame in frame_info]\n",
    "\n",
    "    # Get the audio for each frame\n",
    "    audio_frames = []\n",
    "    for frame_timestamp in frame_timestamps:\n",
    "        audio_start = frame_timestamp - 0.5\n",
    "        audio_end = frame_timestamp + 0.5\n",
    "        audio_start_idx = int(audio_start * audio_rate)\n",
    "        audio_end_idx = int(audio_end * audio_rate)\n",
    "        audio_frames.append(audio[audio_start_idx:audio_end_idx])\n",
    "        break\n",
    "    # Get the material name for each frame\n",
    "    material_names = [frame[1] for frame in frame_info]\n",
    "\n",
    "    # Get the video frames\n",
    "    video_frames = []\n",
    "    for frame_timestamp in frame_timestamps:\n",
    "        frame_idx = int(frame_timestamp * video_rate)\n",
    "        video_frames.append(frames[frame_idx])\n",
    "        break\n",
    "    \n",
    "    # Save the audio, video, and material name\n",
    "    # Create directory for each date_time\n",
    "    os.makedirs(f\"./vis-data-256/{date_time}\", exist_ok=True)\n",
    "    torch.save(audio_frames, f\"./vis-data-256/{date_time}/audio_frames.pt\")\n",
    "    torch.save(video_frames, f\"./vis-data-256/{date_time}/video_frames.pt\")\n",
    "    torch.save(material_names, f\"./vis-data-256/{date_time}/material_names.pt\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_audio_video_material_names(timestamp):\n",
    "    # Read and display the saved audio, video, and material names\n",
    "    audio_frames = torch.load(f\"./vis-data-256/{timestamp}/audio_frames.pt\")\n",
    "    video_frames = torch.load(f\"./vis-data-256/{timestamp}/video_frames.pt\")\n",
    "    material_names = torch.load(f\"./vis-data-256/{timestamp}/material_names.pt\")\n",
    "    print(f\"Date-Time: {timestamp}\")\n",
    "    print(f\"Number of audio frames: {len(audio_frames)}\")\n",
    "    print(f\"Number of video frames: {len(video_frames)}\")\n",
    "    print(f\"Number of material names: {len(material_names)}\")\n",
    "    print(f\"Material names: {material_names}\")\n",
    "    \n",
    "    # Display the first frame, play the first audio, and print the material name\n",
    "    first_frame = video_frames[0]\n",
    "    if first_frame.shape[0] != 3:\n",
    "        first_frame = first_frame.permute(2, 0, 1)\n",
    "    print(f\"First frame: {first_frame.shape}\")\n",
    "    torchvision.io.write_jpeg(first_frame, f\"./vis-data-256/{timestamp}/first_frame.jpg\")\n",
    "    print(f\"Saved first frame to ./vis-data-256/{timestamp}/first_frame.jpg\")\n",
    "    \n",
    "    first_audio = audio_frames[0]\n",
    "    print(f\"First audio: {first_audio.shape}\")\n",
    "    torchaudio.save(f\"./vis-data-256/{timestamp}/first_audio.wav\", first_audio.unsqueeze(0), audio_rate)\n",
    "    print(f\"Saved first audio to ./vis-data-256/{timestamp}/first_audio.wav\")\n",
    "    \n",
    "    print(f\"Material name for first frame: {material_names[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-09-30-20-56-02-193\n",
      "Date-Time: 2015-09-30-20-56-02-193\n",
      "Number of audio frames: 49\n",
      "Number of video frames: 49\n",
      "Number of material names: 49\n",
      "Material names: ['water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water']\n",
      "First frame: torch.Size([3, 256, 456])\n",
      "Saved first frame to ./vis-data-256/2015-09-30-20-56-02-193/first_frame.jpg\n",
      "First audio: torch.Size([96000])\n",
      "Saved first audio to ./vis-data-256/2015-09-30-20-56-02-193/first_audio.wav\n",
      "Material name for first frame: water\n"
     ]
    }
   ],
   "source": [
    "for timestamp, frame_info in times_info.items():\n",
    "    print(timestamp)\n",
    "    save_audio_video_material_names(timestamp)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"./vis-data-256-processed/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_material_annotations = {}\n",
    "for date_time, frame_info in tqdm(times_info.items(), total=len(times_info)):\n",
    "    # Load the audio and video\n",
    "    # video = torch.load(f\"./vis-data-256/{date_time}_denoised.mp4\")\n",
    "    frames, audio, metadata = torchvision.io.read_video(f\"./vis-data-256/{date_time}_denoised.mp4\")\n",
    "    audio = audio[0]\n",
    "    video_rate = metadata[\"video_fps\"]\n",
    "    audio_rate = metadata[\"audio_fps\"]\n",
    "\n",
    "    # Get the timestamps of the frames in the video\n",
    "    frame_timestamps = [frame[0] for frame in frame_info]\n",
    "    material_names = [frame[1] for frame in frame_info]\n",
    "\n",
    "    for i, frame_timestamp in tqdm(enumerate(frame_timestamps), total=len(frame_timestamps), leave=False):\n",
    "        # Save the audio for each frame\n",
    "        audio_start = frame_timestamp - 0.5\n",
    "        audio_end = frame_timestamp + 0.5\n",
    "        audio_start_idx = int(audio_start * audio_rate)\n",
    "        audio_end_idx = int(audio_end * audio_rate)\n",
    "        torch.save(audio[audio_start_idx:audio_end_idx].unsqueeze(0), f\"./vis-data-256-processed/{date_time}_{frame_timestamp}_audio.pt\")\n",
    "\n",
    "        # Save the video frames\n",
    "        frame_idx = int(frame_timestamp * video_rate)\n",
    "        torch.save(frames[frame_idx].permute(2, 0, 1), f\"./vis-data-256-processed/{date_time}_{frame_timestamp}_frame.pt\")\n",
    "\n",
    "        # Add the material name to all_material_annotations\n",
    "        all_material_annotations[f\"{date_time}_{frame_timestamp}\"] = material_names[i]\n",
    "    # break\n",
    "\n",
    "torch.save(all_material_annotations, \"./vis-data-256-processed/all_material_annotations.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-09-30-20-56-02-193_1.647073 water\n"
     ]
    }
   ],
   "source": [
    "annot = torch.load(\"./vis-data-256-processed/all_material_annotations.pt\")\n",
    "for k,v in annot.items():\n",
    "    print(k, v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(glob.glob(\"./vis-data-256/*_times.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 2015-09-30-20-56-02-193\n",
      "len(times_info[key]): 78\n"
     ]
    }
   ],
   "source": [
    "idx = 114\n",
    "\n",
    "for key in times_info.keys():\n",
    "    if idx < len(times_info[key]):\n",
    "        break\n",
    "    else:\n",
    "        idx -= len(times_info[key])\n",
    "        print(f\"Skipping {key}\")\n",
    "key, idx\n",
    "print(f\"len(times_info[key]): {len(times_info[key])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing cv2 for reading frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime: 2015-09-30-20-56-02-193\n"
     ]
    }
   ],
   "source": [
    "for date_time, frame_info in times_info.items():\n",
    "    # Load the audio and video\n",
    "    # video = torch.load(f\"./vis-data-256/{date_time}_denoised.mp4\")\n",
    "    # frames, audio, metadata = torchvision.io.read_video(f\"./vis-data-256/{date_time}_denoised.mp4\")\n",
    "    # audio = audio[0]\n",
    "    # video_rate = metadata[\"video_fps\"]\n",
    "    # audio_rate = metadata[\"audio_fps\"]\n",
    "    print(f\"datetime: {date_time}\")\n",
    "    cap = cv2.VideoCapture(f\"./vis-data-256/{date_time}_denoised.mp4\")\n",
    "    \n",
    "    audio, audio_rate = torchaudio.load(f\"./vis-data-256/{date_time}_denoised.wav\")    \n",
    "    audio = audio[0]\n",
    "    \n",
    "    # Get the timestamps of the frames in the video\n",
    "    frame_timestamps = [frame[0] for frame in frame_info]\n",
    "\n",
    "    # Get the audio for each frame\n",
    "    audio_frames = []\n",
    "    for frame_timestamp in frame_timestamps:\n",
    "        audio_start = frame_timestamp - 0.5\n",
    "        audio_end = frame_timestamp + 0.5\n",
    "        audio_start_idx = int(audio_start * audio_rate)\n",
    "        audio_end_idx = int(audio_end * audio_rate)\n",
    "        audio_frames.append(audio[audio_start_idx:audio_end_idx])\n",
    "        break\n",
    "    # Get the material name for each frame\n",
    "    material_names = [frame[1] for frame in frame_info]\n",
    "\n",
    "    # Get the video frames\n",
    "    video_frames = []\n",
    "    for frame_timestamp in frame_timestamps:\n",
    "        cap.set(cv2.CAP_PROP_POS_MSEC, frame_timestamp * 1000)\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = (T.ToTensor()(frame) * 255).to(torch.uint8)\n",
    "        video_frames.append(frame)\n",
    "        break\n",
    "    # Save the audio, video, and material name\n",
    "    # Create directory for each date_time\n",
    "    cap.release()\n",
    "    os.makedirs(f\"./vis-data-256/{date_time}\", exist_ok=True)\n",
    "    torch.save(audio_frames, f\"./vis-data-256/{date_time}/audio_frames.pt\")\n",
    "    torch.save(video_frames, f\"./vis-data-256/{date_time}/video_frames.pt\")\n",
    "    torch.save(material_names, f\"./vis-data-256/{date_time}/material_names.pt\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-09-30-20-56-02-193\n",
      "Date-Time: 2015-09-30-20-56-02-193\n",
      "Number of audio frames: 1\n",
      "Number of video frames: 1\n",
      "Number of material names: 49\n",
      "Material names: ['water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water']\n",
      "First frame: torch.Size([3, 256, 456])\n",
      "Saved first frame to ./vis-data-256/2015-09-30-20-56-02-193/first_frame.jpg\n",
      "First audio: torch.Size([96000])\n",
      "Saved first audio to ./vis-data-256/2015-09-30-20-56-02-193/first_audio.wav\n",
      "Material name for first frame: water\n"
     ]
    }
   ],
   "source": [
    "for timestamp, frame_info in times_info.items():\n",
    "    print(timestamp)\n",
    "    save_audio_video_material_names(timestamp)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mel spec test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = \"2015-09-29-16-35-56-1\"\n",
    "frame_timestamp = 30.826166\n",
    "audio_length = 5\n",
    "audio_rate = 96000\n",
    "\n",
    "audio = whisper.load_audio(f\"../GreatestHits/vis-data-256/{date_time}_denoised.wav\", 96000)\n",
    "\n",
    "audio_start_time = frame_timestamp - audio_length / 2\n",
    "audio_start_idx = int(audio_start_time * audio_rate)\n",
    "audio2 = audio[audio_start_idx : audio_start_idx + audio_rate * audio_length]\n",
    "audio3 = whisper.pad_or_trim(audio2, audio_rate * audio_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio3.shape[0]/96000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Normalization Parameters over the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset size: 28444\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec279bbd58344e4a4c0f21462e9006c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0, mean: tensor([0.4936, 0.4474, 0.3354]), std: tensor([0.2400, 0.2252, 0.1811])\n",
      "i: 20, mean: tensor([0.4905, 0.4470, 0.3886]), std: tensor([0.2041, 0.1950, 0.1863])\n",
      "i: 40, mean: tensor([0.4803, 0.4428, 0.3825]), std: tensor([0.1968, 0.1877, 0.1787])\n",
      "i: 60, mean: tensor([0.4730, 0.4389, 0.3817]), std: tensor([0.1972, 0.1879, 0.1781])\n",
      "i: 80, mean: tensor([0.4704, 0.4383, 0.3829]), std: tensor([0.1961, 0.1870, 0.1777])\n",
      "i: 100, mean: tensor([0.4708, 0.4368, 0.3799]), std: tensor([0.1983, 0.1898, 0.1803])\n",
      "i: 120, mean: tensor([0.4711, 0.4342, 0.3770]), std: tensor([0.1993, 0.1903, 0.1812])\n",
      "i: 140, mean: tensor([0.4710, 0.4334, 0.3748]), std: tensor([0.2000, 0.1908, 0.1810])\n",
      "i: 160, mean: tensor([0.4683, 0.4290, 0.3708]), std: tensor([0.2002, 0.1915, 0.1821])\n",
      "i: 180, mean: tensor([0.4667, 0.4274, 0.3697]), std: tensor([0.2007, 0.1920, 0.1824])\n",
      "i: 200, mean: tensor([0.4663, 0.4259, 0.3679]), std: tensor([0.2006, 0.1919, 0.1815])\n",
      "i: 220, mean: tensor([0.4672, 0.4264, 0.3681]), std: tensor([0.2003, 0.1917, 0.1814])\n",
      "i: 240, mean: tensor([0.4662, 0.4253, 0.3687]), std: tensor([0.2005, 0.1918, 0.1818])\n",
      "i: 260, mean: tensor([0.4656, 0.4246, 0.3684]), std: tensor([0.2003, 0.1917, 0.1817])\n",
      "i: 280, mean: tensor([0.4669, 0.4264, 0.3698]), std: tensor([0.1996, 0.1912, 0.1811])\n",
      "i: 300, mean: tensor([0.4677, 0.4276, 0.3704]), std: tensor([0.1998, 0.1914, 0.1813])\n",
      "i: 320, mean: tensor([0.4683, 0.4276, 0.3702]), std: tensor([0.1995, 0.1913, 0.1812])\n",
      "i: 340, mean: tensor([0.4690, 0.4277, 0.3701]), std: tensor([0.1988, 0.1908, 0.1805])\n",
      "i: 360, mean: tensor([0.4702, 0.4290, 0.3709]), std: tensor([0.1996, 0.1915, 0.1812])\n",
      "i: 380, mean: tensor([0.4715, 0.4292, 0.3705]), std: tensor([0.1994, 0.1913, 0.1809])\n",
      "i: 400, mean: tensor([0.4717, 0.4292, 0.3703]), std: tensor([0.1996, 0.1915, 0.1810])\n",
      "i: 420, mean: tensor([0.4711, 0.4291, 0.3701]), std: tensor([0.1993, 0.1913, 0.1807])\n",
      "i: 440, mean: tensor([0.4711, 0.4291, 0.3702]), std: tensor([0.1990, 0.1910, 0.1805])\n",
      "i: 460, mean: tensor([0.4712, 0.4289, 0.3701]), std: tensor([0.1996, 0.1915, 0.1809])\n",
      "i: 480, mean: tensor([0.4714, 0.4292, 0.3706]), std: tensor([0.1997, 0.1917, 0.1811])\n",
      "i: 500, mean: tensor([0.4720, 0.4300, 0.3710]), std: tensor([0.1999, 0.1919, 0.1813])\n",
      "i: 520, mean: tensor([0.4719, 0.4295, 0.3706]), std: tensor([0.1998, 0.1919, 0.1811])\n",
      "i: 540, mean: tensor([0.4715, 0.4291, 0.3705]), std: tensor([0.1997, 0.1918, 0.1810])\n",
      "i: 560, mean: tensor([0.4705, 0.4283, 0.3702]), std: tensor([0.1999, 0.1918, 0.1811])\n",
      "i: 580, mean: tensor([0.4705, 0.4282, 0.3700]), std: tensor([0.2001, 0.1920, 0.1813])\n",
      "i: 600, mean: tensor([0.4707, 0.4282, 0.3699]), std: tensor([0.2002, 0.1922, 0.1814])\n",
      "i: 620, mean: tensor([0.4715, 0.4291, 0.3709]), std: tensor([0.2003, 0.1923, 0.1817])\n",
      "i: 640, mean: tensor([0.4713, 0.4291, 0.3710]), std: tensor([0.2001, 0.1923, 0.1817])\n",
      "i: 660, mean: tensor([0.4715, 0.4292, 0.3713]), std: tensor([0.1998, 0.1920, 0.1815])\n",
      "i: 680, mean: tensor([0.4719, 0.4294, 0.3711]), std: tensor([0.1998, 0.1920, 0.1815])\n",
      "i: 700, mean: tensor([0.4725, 0.4298, 0.3714]), std: tensor([0.1997, 0.1918, 0.1813])\n",
      "i: 720, mean: tensor([0.4727, 0.4298, 0.3712]), std: tensor([0.1995, 0.1915, 0.1811])\n",
      "i: 740, mean: tensor([0.4729, 0.4300, 0.3716]), std: tensor([0.1994, 0.1916, 0.1810])\n",
      "i: 760, mean: tensor([0.4727, 0.4300, 0.3714]), std: tensor([0.1994, 0.1916, 0.1810])\n",
      "i: 780, mean: tensor([0.4727, 0.4300, 0.3712]), std: tensor([0.1991, 0.1914, 0.1809])\n",
      "i: 800, mean: tensor([0.4726, 0.4297, 0.3709]), std: tensor([0.1990, 0.1912, 0.1806])\n",
      "i: 820, mean: tensor([0.4723, 0.4295, 0.3707]), std: tensor([0.1992, 0.1913, 0.1807])\n",
      "i: 840, mean: tensor([0.4727, 0.4297, 0.3707]), std: tensor([0.1994, 0.1915, 0.1808])\n",
      "i: 860, mean: tensor([0.4726, 0.4297, 0.3706]), std: tensor([0.1995, 0.1915, 0.1808])\n",
      "i: 880, mean: tensor([0.4729, 0.4300, 0.3707]), std: tensor([0.1995, 0.1915, 0.1808])\n",
      "i: 900, mean: tensor([0.4731, 0.4300, 0.3706]), std: tensor([0.1995, 0.1915, 0.1807])\n",
      "i: 920, mean: tensor([0.4737, 0.4303, 0.3708]), std: tensor([0.1993, 0.1913, 0.1806])\n",
      "i: 940, mean: tensor([0.4737, 0.4305, 0.3707]), std: tensor([0.1993, 0.1914, 0.1806])\n",
      "i: 960, mean: tensor([0.4735, 0.4303, 0.3705]), std: tensor([0.1992, 0.1912, 0.1805])\n",
      "i: 980, mean: tensor([0.4736, 0.4303, 0.3705]), std: tensor([0.1991, 0.1911, 0.1804])\n",
      "i: 1000, mean: tensor([0.4738, 0.4303, 0.3703]), std: tensor([0.1992, 0.1912, 0.1804])\n",
      "i: 1020, mean: tensor([0.4739, 0.4303, 0.3703]), std: tensor([0.1989, 0.1908, 0.1801])\n",
      "i: 1040, mean: tensor([0.4739, 0.4303, 0.3703]), std: tensor([0.1989, 0.1909, 0.1802])\n",
      "i: 1060, mean: tensor([0.4737, 0.4303, 0.3706]), std: tensor([0.1991, 0.1911, 0.1803])\n",
      "i: 1080, mean: tensor([0.4739, 0.4302, 0.3704]), std: tensor([0.1992, 0.1912, 0.1803])\n",
      "i: 1100, mean: tensor([0.4740, 0.4301, 0.3702]), std: tensor([0.1990, 0.1909, 0.1800])\n",
      "i: 1120, mean: tensor([0.4742, 0.4302, 0.3703]), std: tensor([0.1992, 0.1912, 0.1803])\n",
      "i: 1140, mean: tensor([0.4739, 0.4300, 0.3703]), std: tensor([0.1992, 0.1912, 0.1804])\n",
      "i: 1160, mean: tensor([0.4742, 0.4300, 0.3702]), std: tensor([0.1993, 0.1912, 0.1803])\n",
      "i: 1180, mean: tensor([0.4738, 0.4298, 0.3702]), std: tensor([0.1994, 0.1913, 0.1804])\n",
      "mean: tensor([0.4738, 0.4298, 0.3702]), std: tensor([0.1994, 0.1913, 0.1805])\n"
     ]
    }
   ],
   "source": [
    "from dataset_utils import create_dataloaders\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 4\n",
    "dataloader = create_dataloaders(root_dir=\"/home/GreatestHits/vis-data-256\", batch_size=batch_size)\n",
    "\n",
    "# Iterate over the dataset to determine normalization parameters for the images\n",
    "mean = 0.\n",
    "std = 0.\n",
    "nb_samples = 0.\n",
    "for i, batch in tqdm(enumerate(dataloader), total=len(dataloader)//6):\n",
    "    data = batch[\"images\"]\n",
    "    batch_samples = data.size(0)\n",
    "    data = data.view(batch_samples, data.size(1), -1)\n",
    "    mean += data.mean(2).sum(0)\n",
    "    std += data.std(2).sum(0)\n",
    "    nb_samples += batch_samples\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(f\"i: {i}, mean: {mean/nb_samples}, std: {std/nb_samples}\")\n",
    "    \n",
    "    if i == len(dataloader)//6 - 1:\n",
    "        break\n",
    "    \n",
    "mean /= nb_samples\n",
    "std /= nb_samples\n",
    "print(f\"mean: {mean}, std: {std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all material names, create dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset size: 28444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataset_utils import create_dataloaders\n",
    "loader = create_dataloaders(root_dir=\"/home/GreatestHits/vis-data-256\", batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'carpet': 0,\n",
       " 'ceramic': 1,\n",
       " 'cloth': 2,\n",
       " 'dirt': 3,\n",
       " 'drywall': 4,\n",
       " 'glass': 5,\n",
       " 'grass': 6,\n",
       " 'gravel': 7,\n",
       " 'leaf': 8,\n",
       " 'metal': 9,\n",
       " 'paper': 10,\n",
       " 'plastic': 11,\n",
       " 'plastic-bag': 12,\n",
       " 'rock': 13,\n",
       " 'tile': 14,\n",
       " 'water': 15,\n",
       " 'wood': 16}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader.dataset.times_info)\n",
    "loader.dataset.material_names_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist = torch.load(\"loss_history.pth\")\n",
    "loss_hist_ema = []\n",
    "l = loss_hist[0]\n",
    "a = 0.999\n",
    "for val in loss_hist:\n",
    "    l = a * l + (1 - a) * val\n",
    "    loss_hist_ema.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,1,1)\n",
    "plt.title(\"EMA Loss\")\n",
    "plt.plot(loss_hist_ema)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_hist)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Balanced Sampler tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataset_utils import GreatestHitsDataset\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassBalancedSampler(Sampler):\n",
    "    def __init__(self, data_source, num_classes, batch_size):\n",
    "        self.data_source = data_source\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.class_indices = self._create_class_indices()\n",
    "        \n",
    "    def _create_class_indices(self):\n",
    "        class_indices = [[] for _ in range(self.num_classes)]\n",
    "        for idx, data in tqdm(enumerate(self.data_source), total=len(self.data_source)):\n",
    "            class_indices[data[\"materials\"].item()].append(idx)\n",
    "        return class_indices\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        class_indices_copy = [indices[:] for indices in self.class_indices]\n",
    "        for i in range(len(self.data_source) // self.batch_size):\n",
    "            chosen_classes = random.sample(range(self.num_classes), self.batch_size)\n",
    "            for cls in chosen_classes:\n",
    "                if not class_indices_copy[cls]:\n",
    "                    class_indices_copy[cls] = self.class_indices[cls][:]\n",
    "                    random.shuffle(class_indices_copy[cls])\n",
    "                batch.append(class_indices_copy[cls].pop())\n",
    "            yield batch\n",
    "            batch = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_source) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = GreatestHitsDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cbs = ClassBalancedSampler(dataset, len(dataset.all_material_names), 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 10, time so far = 6.58697772026062s\n",
      "Done with 20, time so far = 12.785082578659058s\n",
      "Done with 30, time so far = 18.992053508758545s\n",
      "Done with 40, time so far = 25.788207292556763s\n",
      "Done with 50, time so far = 31.888122081756592s\n",
      "Done with 60, time so far = 37.0326464176178s\n",
      "Done with 70, time so far = 43.7861385345459s\n",
      "Done with 80, time so far = 49.1879608631134s\n",
      "Done with 90, time so far = 54.75180435180664s\n",
      "Done with 100, time so far = 60.684203147888184s\n",
      "Done with 110, time so far = 66.88485336303711s\n",
      "Done with 120, time so far = 72.78800654411316s\n",
      "Done with 130, time so far = 79.38831949234009s\n",
      "Done with 140, time so far = 85.7841329574585s\n",
      "Done with 150, time so far = 92.19615244865417s\n",
      "Done with 160, time so far = 98.28623056411743s\n",
      "Done with 170, time so far = 103.58437252044678s\n",
      "Done with 180, time so far = 109.5655152797699s\n",
      "Done with 190, time so far = 116.68798184394836s\n",
      "Done with 200, time so far = 122.08775305747986s\n",
      "Done with 210, time so far = 128.47720789909363s\n",
      "Done with 220, time so far = 134.9839973449707s\n",
      "Done with 230, time so far = 140.68657445907593s\n",
      "Done with 240, time so far = 146.06630516052246s\n",
      "Done with 250, time so far = 151.79352569580078s\n",
      "Done with 260, time so far = 157.78667330741882s\n",
      "Done with 270, time so far = 163.7839915752411s\n",
      "Done with 280, time so far = 170.18964958190918s\n",
      "Done with 290, time so far = 176.2848334312439s\n",
      "Done with 300, time so far = 182.3950638771057s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_45316/2149421373.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mclass_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"materials\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mat_est_vol/MultiModalMaterialEstimation/dataset_utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mmel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;31m# Load jpg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.dataset_cache_dir}/{date_time}_{frame_timestamp}_frame.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class_counts = [0] * len(dataset.all_material_names)\n",
    "i = 0\n",
    "for data in dataset:\n",
    "    class_counts[data[\"materials\"].item()] += 1\n",
    "    i += 1\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Done with {i}, time so far = {time.time() - start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
